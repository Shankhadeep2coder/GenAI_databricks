{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb9af0c-1213-4ee2-96e6-a65afef85dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install databricks-vectorsearch\n",
    "%pip install --upgrade langgraph langchain_core databricks-langchain mlflow-skinny[databricks]\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25516b11-c77b-48ea-a107-638a4061d7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.schema import HumanMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a33e9f7-6361-43c0-9af9-578ed8af1c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Connect to your Vector Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8a418d-10e7-4e62-b5b7-e1b6c1e27f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vsc = VectorSearchClient()\n",
    "\n",
    "index = vsc.get_index(\n",
    "    index_name=\"genai.default.vec_index\",  # fully qualified index name\n",
    "    endpoint_name=\"genai_vec_search_shankho\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3846749b-a5ef-4a2f-936b-a7708d35881c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3: Define function to embed the question (same model as indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7c68cf-3a1e-4bb7-9b4e-a72b61e6c1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "embedding_client = get_deploy_client(\"databricks\")\n",
    "embedding_endpoint = \"databricks-bge-large-en\"\n",
    "\n",
    "def get_question_embedding(question: str):\n",
    "    res = embedding_client.predict(endpoint=embedding_endpoint, inputs={\"input\": [question]})\n",
    "    return res[\"data\"][0][\"embedding\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50ac7d33-5879-4f4f-96a1-6860893d966e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4: Perform vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20bb5c59-2ccb-4450-95b1-5715c7445dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(question: str, k: int = 4):\n",
    "    # 1. Embed the question\n",
    "    query_vector = get_question_embedding(question)\n",
    "\n",
    "    # 2. Perform vector similarity search\n",
    "    response = index.similarity_search(\n",
    "        query_vector=query_vector,\n",
    "        columns=[\"chunk_id\", \"content\", \"source\"],\n",
    "        num_results=k\n",
    "    )\n",
    "\n",
    "    # 3. Extract content from results (which is a dict)\n",
    "    results = response.get(\"results\", [])\n",
    "    chunks = [row[\"content\"] for row in results if \"content\" in row]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "546d1d52-0e4e-4d02-b242-f05064f97a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5: Call the LLM with retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8374369c-8453-459a-a321-95683bfc2070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\")  # Or any other model you use\n",
    "\n",
    "def generate_answer(question: str):\n",
    "    context_chunks = retrieve_relevant_chunks(question)\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "    \n",
    "                    Context:\n",
    "                    {context}\n",
    "\n",
    "                    Question: {question}\n",
    "                    Answer:\"\"\"\n",
    "\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7547af2-88ef-44f3-919f-ceae9f7356a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d001d8-5be6-431f-ab35-a336362a5e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is tanswer for Queston The course will equip you to provide care while waiting for?\"\n",
    "answer = generate_answer(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bot_QnA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
