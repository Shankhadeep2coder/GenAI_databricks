{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7f9480-acad-41b7-accb-4730fc314c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 1: Read PDFs from Unity Catalog Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dfc7e7f4-61fc-48ef-acb8-74e4ff85aa84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66311d87-3d14-485b-b6c4-b8f0d912ef92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import fitz  # PyMuPDF\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import os\n",
    "\n",
    "volume_path = \"dbfs:/Volumes/genai/default/pdf_store/\"\n",
    "pdf_files = [f.path for f in dbutils.fs.ls(volume_path) if f.path.endswith(\".pdf\")]\n",
    "\n",
    "print(pdf_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adbb765e-c568-4b7c-8d92-bb454f17e609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Parse and Chunk PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7075df80-b22a-45ba-a6e1-77dda84dc6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from io import BytesIO\n",
    "import uuid\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define table schema (once, before first write)\n",
    "schema = StructType([\n",
    "    StructField(\"chunk_id\", StringType(), False),\n",
    "    StructField(\"source\", StringType(), False),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Create empty table only if needed (run once)\n",
    "spark.createDataFrame([], schema).write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"genai.default.pdf_chunks\")\n",
    "\n",
    "# Text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Function to process a PDF from Unity Catalog volume\n",
    "def chunk_pdf_to_rows(dbfs_path):\n",
    "    try:\n",
    "        # Read PDF as binary\n",
    "        spark_df = spark.read.format(\"binaryFile\").load(dbfs_path)\n",
    "        file_bytes = spark_df.select(\"content\").first()[\"content\"]\n",
    "\n",
    "        with BytesIO(file_bytes) as f:\n",
    "            doc = fitz.open(stream=f.read(), filetype=\"pdf\")\n",
    "            text = \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = text_splitter.create_documents([text])\n",
    "\n",
    "        # Add metadata manually\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata = {\"source\": dbfs_path}\n",
    "\n",
    "        # Convert to Spark rows\n",
    "        rows = [\n",
    "            Row(\n",
    "                chunk_id=str(uuid.uuid4()),\n",
    "                source=chunk.metadata[\"source\"],\n",
    "                content=chunk.page_content\n",
    "            )\n",
    "            for chunk in chunks\n",
    "        ]\n",
    "        return rows\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {dbfs_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Define volume path\n",
    "volume_path = \"dbfs:/Volumes/genai/default/pdf_store/\"\n",
    "\n",
    "# List all PDFs\n",
    "pdf_files = [f.path for f in dbutils.fs.ls(volume_path) if f.path.endswith(\".pdf\")]\n",
    "\n",
    "# Aggregate all rows\n",
    "all_rows = []\n",
    "for pdf_path in pdf_files:\n",
    "    rows = chunk_pdf_to_rows(pdf_path)\n",
    "    all_rows.extend(rows)\n",
    "\n",
    "# Write to Delta only if there is data\n",
    "if all_rows:\n",
    "    df = spark.createDataFrame(all_rows, schema=schema)\n",
    "    df.write.mode(\"append\").format(\"delta\").saveAsTable(\"genai.default.pdf_chunks\")\n",
    "    print(f\"✅ Successfully ingested {len(all_rows)} chunks into genai.default.pdf_chunks\")\n",
    "else:\n",
    "    print(\"⚠️ No valid PDF chunks found to write.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e00f027-8eae-4298-86b5-89829f9d0b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 3: Code to Embed Your Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf6229b-14b7-428c-8189-8d7f59db7ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"\n",
    "\n",
    "# Define a batch embedding function for a pandas Series\n",
    "def get_embeddings_batch(text_series: pd.Series) -> pd.Series:\n",
    "    # Batch call\n",
    "    response = client.predict(endpoint=EMBEDDING_ENDPOINT_NAME, inputs={\"input\": text_series.tolist()})\n",
    "    return pd.Series([embedding for embedding in response[\"embeddings\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24461a49-f08a-4473-bd67-cc749be55ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 4: Add Embeddings to Delta Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca39c02-640f-4036-97a9-f5f4d7e8ff3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_DEBUGGING Snippet\n",
    "Pull a small batch_\n",
    "sample_df = spark.table(\"genai.default.pdf_chunks\").limit(2).toPandas()\n",
    "\n",
    "_Define a local (non-UDF) embedding function_\n",
    "def local_embed(text):\n",
    "    print(f\"Embedding: {text[:50]}...\")  # Short preview\n",
    "    response = client.predict(endpoint=EMBEDDING_ENDPOINT_NAME, inputs={\"input\": [text]})\n",
    "    return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "sample_df[\"embedding\"] = sample_df[\"content\"].apply(local_embed)\n",
    "print(sample_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ddc829-ada5-4caf-8c7a-32ef0251e2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "import pandas as pd\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "endpoint = \"databricks-bge-large-en\"\n",
    "\n",
    "def get_embeddings_batch(texts):\n",
    "    response = client.predict(endpoint=endpoint, inputs={\"input\": texts})\n",
    "\n",
    "    if \"data\" not in response:\n",
    "        raise ValueError(f\"'data' key not found in response: {response}\")\n",
    "\n",
    "    # Extract embeddings list\n",
    "    return [item[\"embedding\"] for item in response[\"data\"]]\n",
    "\n",
    "# Fetch already embedded chunk_ids to skip\n",
    "try:\n",
    "    embedded_chunk_ids = spark.table(\"genai.default.pdf_chunks_embedded\") \\\n",
    "                               .select(\"chunk_id\") \\\n",
    "                               .toPandas()[\"chunk_id\"] \\\n",
    "                               .tolist()\n",
    "except:\n",
    "    embedded_chunk_ids = []\n",
    "\n",
    "# Read original chunks table in batches\n",
    "batch_size = 100\n",
    "total_rows = spark.sql(\"SELECT COUNT(*) FROM genai.default.pdf_chunks\").collect()[0][0]\n",
    "\n",
    "for offset in range(0, total_rows, batch_size):\n",
    "    batch_df = spark.sql(f\"\"\"\n",
    "        SELECT * FROM genai.default.pdf_chunks\n",
    "        ORDER BY chunk_id\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "    \"\"\").toPandas()\n",
    "\n",
    "    # Filter out already embedded chunk_ids\n",
    "    batch_df = batch_df[~batch_df[\"chunk_id\"].isin(embedded_chunk_ids)]\n",
    "\n",
    "    if batch_df.empty:\n",
    "        print(f\"Skipping offset {offset} (already embedded)\")\n",
    "        continue\n",
    "\n",
    "    # Generate embeddings\n",
    "    batch_df[\"embedding\"] = get_embeddings_batch(batch_df[\"content\"].tolist())\n",
    "\n",
    "    # Write to embedded table\n",
    "    spark.createDataFrame(batch_df).write.mode(\"append\").format(\"delta\").saveAsTable(\"genai.default.pdf_chunks_embedded\")\n",
    "\n",
    "    print(f\"✅ Embedded and stored batch offset {offset} ({len(batch_df)} rows)\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "vec_embedding",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
